# üìà Reinforcement Learning para Trading Cuantitativo
# Este notebook implementa un agente de RL para optimizar estrategias de trading.

# 1Ô∏è‚É£ Importar librer√≠as necesarias
import pandas as pd
import numpy as np
import gym
import matplotlib.pyplot as plt
from stable_baselines3 import PPO
from stable_baselines3.common.env_util import make_vec_env
from gym import spaces

# 2Ô∏è‚É£ Descargar datos hist√≥ricos (Ejemplo: AAPL)
import yfinance as yf
ticker = "AAPL"
data = yf.download(ticker, start="2020-01-01", end="2024-01-01")

# 3Ô∏è‚É£ Definir el entorno personalizado para Reinforcement Learning
class TradingEnv(gym.Env):
    def __init__(self, data):
        super(TradingEnv, self).__init__()
        self.data = data
        self.current_step = 0

        # Espacio de acciones: 0 = Mantener, 1 = Comprar, 2 = Vender
        self.action_space = spaces.Discrete(3)

        # Espacio de observaci√≥n: Precio de cierre + indicadores t√©cnicos
        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(3,), dtype=np.float32)

    def reset(self):
        self.current_step = 0
        return self._next_observation()

    def _next_observation(self):
        return np.array([
            self.data["Adj Close"].iloc[self.current_step],
            self.data["Volume"].iloc[self.current_step],
            self.data["Adj Close"].pct_change().iloc[self.current_step]
        ])

    def step(self, action):
        self.current_step += 1
        done = self.current_step >= len(self.data) - 1
        reward = self._calculate_reward(action)
        return self._next_observation(), reward, done, {}

    def _calculate_reward(self, action):
        if action == 1:  # Comprar
            return self.data["Adj Close"].pct_change().iloc[self.current_step] * 100
        elif action == 2:  # Vender
            return -self.data["Adj Close"].pct_change().iloc[self.current_step] * 100
        else:
            return 0  # Mantener

# 4Ô∏è‚É£ Inicializar el entorno de trading
env = TradingEnv(data)
vec_env = make_vec_env(lambda: env, n_envs=1)

# 5Ô∏è‚É£ Entrenar el modelo de Reinforcement Learning con PPO
model = PPO("MlpPolicy", vec_env, verbose=1)
model.learn(total_timesteps=10_000)

# 6Ô∏è‚É£ Evaluar la estrategia entrenada
obs = env.reset()
rewards = []

for _ in range(100):
    action, _states = model.predict(obs)
    obs, reward, done, _ = env.step(action)
    rewards.append(reward)
    if done:
        break

# üî• Visualizar resultados
plt.plot(rewards)
plt.title("Recompensa acumulada durante la simulaci√≥n")
plt.xlabel("Tiempo")
plt.ylabel("Recompensa")
plt.show()
